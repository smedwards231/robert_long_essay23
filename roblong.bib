
@article{ferruz_towards_2022,
	title = {Towards {Controllable} {Protein} {Design} with {Conditional} {Transformers}},
	language = {en},
	author = {Ferruz, Noelia and Höcker, Birte},
	year = {2022},
	file = {Ferruz and Höcker - Towards Controllable Protein Design with Condition.pdf:C\:\\Users\\sydne\\Zotero\\storage\\LAXEQAAU\\Ferruz and Höcker - Towards Controllable Protein Design with 
Condition.pdf:application/pdf},
}

@article{fleming_secondary_2006,
	title = {Secondary structure determines protein topology},
	volume = {15},
	issn = {1469-896X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1110/ps.062305106},
	doi = {10.1110/ps.062305106},
	abstract = {Using a test set of 13 small, compact proteins, we demonstrate that a remarkably simple protocol can capture native topology from secondary structure information alone, in the 
absence of long-range interactions. It has been a long-standing open question whether such information is sufficient to determine a protein's fold. Indeed, even the far simpler problem of reconstructing 
the three-dimensional structure of a protein from its exact backbone torsion angles has remained a difficult challenge owing to the small, but cumulative, deviations from ideality in backbone planarity, 
which, if ignored, cause large errors in structure. As a familiar example, a small change in an elbow angle causes a large displacement at the end of your arm; the longer the arm, the larger the 
displacement. Here, correct secondary structure assignments (α-helix, β-strand, β-turn, polyproline II, coil) were used to constrain polypeptide backbone chains devoid of side chains, and the most 
stable folded conformations were determined, using Monte Carlo simulation. Just three terms were used to assess stability: molecular compaction, steric exclusion, and hydrogen bonding. For nine of the 
13 proteins, this protocol restricts the main chain to a surprisingly small number of energetically favorable topologies, with the native one prominent among them.},
	language = {en},
	number = {8},
	urldate = {2023-03-31},
	journal = {Protein Science},
	author = {Fleming, Patrick J. and Gong, Haipeng and Rose, George D.},
	year = {2006},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1110/ps.062305106},
	keywords = {confinement, hydrogen bonding, protein folding, protein topology, secondary structure},
	pages = {1829--1834},
	file = {Full Text PDF:C\:\\Users\\sydne\\Zotero\\storage\\93KL7BT8\\Fleming et al. - 2006 - Secondary structure determines protein 
topology.pdf:application/pdf;Snapshot:C\:\\Users\\sydne\\Zotero\\storage\\B6SPDLSN\\ps.html:text/html},
}

@article{mizutani_relating_2012,
	title = {Relating drug–protein interaction network with drug side effects},
	volume = {28},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/bts383},
	doi = {10.1093/bioinformatics/bts383},
	abstract = {Motivation: Identifying the emergence and underlying mechanisms of drug side effects is a challenging task in the drug development process. This underscores the importance of 
system–wide approaches for linking different scales of drug actions; namely drug-protein interactions (molecular scale) and side effects (phenotypic scale) toward side effect prediction for 
uncharacterized drugs.Results: We performed a large-scale analysis to extract correlated sets of targeted proteins and side effects, based on the co-occurrence of drugs in protein-binding profiles and 
side effect profiles, using sparse canonical correlation analysis. The analysis of 658 drugs with the two profiles for 1368 proteins and 1339 side effects led to the extraction of 80 correlated sets. 
Enrichment analyses using KEGG and Gene Ontology showed that most of the correlated sets were significantly enriched with proteins that are involved in the same biological pathways, even if their 
molecular functions are different. This allowed for a biologically relevant interpretation regarding the relationship between drug–targeted proteins and side effects. The extracted side effects can be 
regarded as possible phenotypic outcomes by drugs targeting the proteins that appear in the same correlated set. The proposed method is expected to be useful for predicting potential side effects of new 
drug candidate compounds based on their protein-binding profiles.Supplementary information: Datasets and all results are available at 
http://web.kuicr.kyoto-u.ac.jp/supp/smizutan/target-effect/.Availability: Software is available at the above supplementary website.Contact:  yamanishi@bioreg.kyushu-u.ac.jp, or 
goto@kuicr.kyoto-u.ac.jp},
	number = {18},
	urldate = {2023-03-31},
	journal = {Bioinformatics},
	author = {Mizutani, Sayaka and Pauwels, Edouard and Stoven, Véronique and Goto, Susumu and Yamanishi, Yoshihiro},
	month = sep,
	year = {2012},
	pages = {i522--i528},
	file = {Full Text PDF:C\:\\Users\\sydne\\Zotero\\storage\\8ZPSJ5XK\\Mizutani et al. - 2012 - Relating drug–protein interaction network with 
dru.pdf:application/pdf;Snapshot:C\:\\Users\\sydne\\Zotero\\storage\\YXDDUL2D\\246017.html:text/html},
}

@article{ferruz_protgpt2_2022,
	title = {{ProtGPT2} is a deep unsupervised language model for protein design},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-32007-7},
	doi = {10.1038/s41467-022-32007-7},
	abstract = {Protein design aims to build novel proteins customized for specific purposes, thereby holding the potential to tackle many environmental and biomedical problems. Recent progress in 
Transformer-based architectures has enabled the implementation of language models capable of generating text with human-like capabilities. Here, motivated by this success, we describe ProtGPT2, a 
language model trained on the protein space that generates de novo protein sequences following the principles of natural ones. The generated proteins display natural amino acid propensities, while 
disorder predictions indicate that 88\% of ProtGPT2-generated proteins are globular, in line with natural sequences. Sensitive sequence searches in protein databases show that ProtGPT2 sequences are 
distantly related to natural ones, and similarity networks further demonstrate that ProtGPT2 is sampling unexplored regions of protein space. AlphaFold prediction of ProtGPT2-sequences yields 
well-folded non-idealized structures with embodiments and large loops and reveals topologies not captured in current structure databases. ProtGPT2 generates sequences in a matter of seconds and is 
freely available.},
	language = {en},
	number = {1},
	urldate = {2023-03-31},
	journal = {Nature Communications},
	author = {Ferruz, Noelia and Schmidt, Steffen and Höcker, Birte},
	month = jul,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational science, Machine learning},
	pages = {4348},
	file = {Full Text PDF:C\:\\Users\\sydne\\Zotero\\storage\\AR4FUMQN\\Ferruz et al. - 2022 - ProtGPT2 is a deep unsupervised language model for.pdf:application/pdf},
}

@article{alva_vocabulary_2015,
	title = {A vocabulary of ancient peptides at the origin of folded proteins},
	volume = {4},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.09410},
	doi = {10.7554/eLife.09410},
	abstract = {The seemingly limitless diversity of proteins in nature arose from only a few thousand domain prototypes, but the origin of these themselves has remained unclear. We are pursuing the 
hypothesis that they arose by fusion and accretion from an ancestral set of peptides active as co-factors in RNA-dependent replication and catalysis. Should this be true, contemporary domains may still 
contain vestiges of such peptides, which could be reconstructed by a comparative approach in the same way in which ancient vocabularies have been reconstructed by the comparative study of modern 
languages. To test this, we compared domains representative of known folds and identified 40 fragments whose similarity is indicative of common descent, yet which occur in domains currently not thought 
to be homologous. These fragments are widespread in the most ancient folds and enriched for iron-sulfur- and nucleic acid-binding. We propose that they represent the observable remnants of a primordial 
RNA-peptide world.},
	urldate = {2023-03-31},
	journal = {eLife},
	author = {Alva, Vikram and Söding, Johannes and Lupas, Andrei N},
	editor = {Kuriyan, John},
	month = dec,
	year = {2015},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {ancient peptides, domain evolution, LUCA, protein evolution, RNA-peptide world},
	pages = {e09410},
	file = {Full Text PDF:C\:\\Users\\sydne\\Zotero\\storage\\3TCGC97R\\Alva et al. - 2015 - A vocabulary of ancient peptides at the origin of .pdf:application/pdf},
}

@article{quaresima_missense_2006,
	title = {Missense mutations of {BRCA1} gene affect the binding with p53 both in vitro and in vivo},
	volume = {16},
	issn = {1021-335X},
	url = {https://www.spandidos-publications.com/10.3892/or.16.4.811},
	doi = {10.3892/or.16.4.811},
	abstract = {Women with BRCA1 gene mutations have an increased risk for breast and ovarian cancer (BOC). Classification of missense variants as neutral or disease causing is still a challenge and 
has major implications for genetic counseling. BRCA1 is organized in an N-terminal ring-finger domain and two BRCT (breast cancer C-terminus) domains, involved in protein-protein interaction. The 
integrity of the C-terminal, BRCT repeat region is also critical for BRCA1 tumor suppressor function. Several molecular partners of BRCA1 have so far been identified; among them, the tumor suppressor 
protein p53 seems to play a major role. This study was aimed at evaluating the impact of two missense mutations, namely the W1837R and the S1841N, previously identified in BOC patients and located in 
the BRCT domain of the BRCA1 gene, on the binding capacity of this protein to p53. Co-immunoprecipitation assays of E. coli-expressed wild-type and mutated BRCTs challenged with a HeLa cell extract 
revealed, for the S1841N variant a significant reduction in the binding activity to p53, while the W1837R mutant showed an inverse effect. Furthermore, a clonogenic soft agar growth assay performed on 
HeLa cells stably transfected with either wild-type or mutant BRCA1 showed a marked decrease of the growth in wild-type BRCA1-overexpressing cells and in BRCA1S1841N-transfected cells, while no 
significant changes were detected in the BRCA1W1837R-transfected cells. These results demonstrate that: i) distinct single nucleotide changes in the BRCT domain of BRCA1 affect binding of this protein 
to the tumor suppressor p53, and ii) the two missense mutations here described are likely to play a role in breast tumorigenesis. We suggest that in vitro/in vivo experiments testing the effects of 
unclassified BRCA1 gene variants should therefore be taken in to consideration and that increased surveillance should be adopted in individuals bearing these two BRCA1 missense alterations.},
	number = {4},
	urldate = {2023-03-31},
	journal = {Oncology Reports},
	author = {Quaresima, Barbara and Faniello, Maria C. and Baudi, Francesco and Crugliano, Telma and Di Sanzo, Maddalena and Cuda, Giovanni and Costanzo, Francesco and Venuta, Salvatore},
	month = oct,
	year = {2006},
	note = {Publisher: Spandidos Publications},
	pages = {811--815},
	file = {Full Text PDF:C\:\\Users\\sydne\\Zotero\\storage\\WBWWEPK3\\Quaresima et al. - 2006 - Missense mutations of BRCA1 gene affect the bindin.pdf:application/pdf},
}

@article{pertsemlidis_having_2001,
	title = {Having a {BLAST} with bioinformatics (and avoiding {BLASTphemy})},
	volume = {2},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/gb-2001-2-10-reviews2002},
	doi = {10.1186/gb-2001-2-10-reviews2002},
	abstract = {Searching for similarities between biological sequences is the principal means by which bioinformatics contributes to our understanding of biology. Of the various informatics tools 
developed to accomplish this task, the most widely used is BLAST, the basic local alignment search tool. This article discusses the principles, workings, applications and potential pitfalls of BLAST, 
focusing on the implementation developed at the National Center for Biotechnology Information.},
	number = {10},
	urldate = {2023-03-31},
	journal = {Genome Biology},
	author = {Pertsemlidis, Alexander and Fondon, John W.},
	month = sep,
	year = {2001},
	keywords = {Basic Local Alignment Search Tool, Good Local Alignment, Query Sequence, Ungapped Alignment, Unrelated Sequence},
	pages = {reviews2002.1},
	file = {Full Text PDF:C\:\\Users\\sydne\\Zotero\\storage\\DKSZXJDX\\Pertsemlidis and Fondon - 2001 - Having a BLAST with bioinformatics (and avoiding 
B.pdf:application/pdf;Snapshot:C\:\\Users\\sydne\\Zotero\\storage\\JLSSVSC4\\gb-2001-2-10-reviews2002.html:text/html},
}

@misc{noauthor_ar-net_nodate,
	title = {{AR}-{Net}: {A} simple autoregressive neural network for time series},
	shorttitle = {{AR}-{Net}},
	url = {https://ai.facebook.com/blog/ar-net-a-simple-autoregressive-neural-network-for-time-series/},
	abstract = {AR-Net is a new framework that combines the best of both traditional statistical models and neural network models for time series modeling. The feed forward model is not only as 
interpretable as AR models but is also scalable and easier to use.},
	language = {en},
	urldate = {2023-03-31},
	file = {Snapshot:C\:\\Users\\sydne\\Zotero\\storage\\HWWDI985\\ar-net-a-simple-autoregressive-neural-network-for-time-series.html:text/html},
}

@misc{triebe_ar-net_2019,
	title = {{AR}-{Net}: {A} simple {Auto}-{Regressive} {Neural} {Network} for time-series},
	shorttitle = {{AR}-{Net}},
	url = {http://arxiv.org/abs/1911.12436},
	abstract = {In this paper we present a new framework for time-series modeling that combines the best of traditional statistical models and neural networks. We focus on time-series with 
long-range dependencies, needed for monitoring fine granularity data (e.g. minutes, seconds, milliseconds), prevalent in operational use-cases. Traditional models, such as auto-regression fitted with 
least squares (Classic-AR) can model time-series with a concise and interpretable model. When dealing with long-range dependencies, Classic-AR models can become intractably slow to fit for large data. 
Recently, sequence-to-sequence models, such as Recurrent Neural Networks, which were originally intended for natural language processing, have become popular for time-series. However, they can be overly 
complex for typical time-series data and lack interpretability. A scalable and interpretable model is needed to bridge the statistical and deep learning-based approaches. As a first step towards this 
goal, we propose modelling AR-process dynamics using a feed-forward neural network approach, termed AR-Net. We show that AR-Net is as interpretable as Classic-AR but also scales to long-range 
dependencies. Our results lead to three major conclusions: First, AR-Net learns identical AR-coefficients as Classic-AR, thus being equally interpretable. Second, the computational complexity with 
respect to the order of the AR process, is linear for AR-Net as compared to a quadratic for Classic-AR. This makes it possible to model long-range dependencies within fine granularity data. Third, by 
introducing regularization, AR-Net automatically selects and learns sparse AR-coefficients. This eliminates the need to know the exact order of the AR-process and allows to learn sparse weights for a 
model with long-range dependencies.},
	urldate = {2023-03-31},
	publisher = {arXiv},
	author = {Triebe, Oskar and Laptev, Nikolay and Rajagopal, Ram},
	month = nov,
	year = {2019},
	note = {arXiv:1911.12436 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Building a bridge between traditional statistical time-series models and deep-learning models. Main Topics: Time-Series, Auto-Regression, Neural Networks, Sparsity, Long-Range 
Dependencies},
	file = {arXiv Fulltext PDF:C\:\\Users\\sydne\\Zotero\\storage\\7YM65J8C\\Triebe et al. - 2019 - AR-Net A simple Auto-Regressive Neural Network fo.pdf:application/pdf;arXiv.org 
Snapshot:C\:\\Users\\sydne\\Zotero\\storage\\U3H8S5PZ\\1911.html:text/html},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	abstract = {This paper rigorously establishes thut standard rnultiluyer feedforward networks with as f\&v us one hidden layer using arbitrary squashing functions ure capable of upproximating uny 
Bore1 measurable function from one finite dimensional space to another to any desired degree of uccuracy, provided sujficirntly muny hidden units are available. In this sense, multilayer feedforward 
networks are u class of universul rlpproximators.},
	language = {en},
	number = {5},
	urldate = {2023-03-31},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	pages = {359--366},
	file = {Hornik et al. - 1989 - Multilayer feedforward networks are universal appr.pdf:C\:\\Users\\sydne\\Zotero\\storage\\VX94B2WP\\Hornik et al. - 1989 - Multilayer feedforward networks are 
universal appr.pdf:application/pdf},
}

@article{sheari_tale_2008,
	title = {A tale of two symmetrical tails: {Structural} and functional characteristics of palindromes in proteins},
	volume = {9},
	shorttitle = {A tale of two symmetrical tails},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2474621/},
	doi = {10.1186/1471-2105-9-274},
	abstract = {It has been previously shown that palindromic sequences are frequently observed in proteins. However, our knowledge about their evolutionary origin and their possible importance is 
incomplete.In this work, we tried to revisit this relatively neglected ...},
	language = {en},
	urldate = {2023-03-31},
	journal = {BMC Bioinformatics},
	author = {Sheari, Armita and Kargar, Mehdi and Katanforoush, Ali and Arab, Shahriar and Sadeghi, Mehdi and Pezeshk, Hamid and Eslahchi, Changiz and Marashi, Sayed-Amir},
	year = {2008},
	pmid = {18547401},
	note = {Publisher: BioMed Central},
	pages = {274},
	file = {Full Text:C\:\\Users\\sydne\\Zotero\\storage\\SFALPM6E\\Sheari et al. - 2008 - A tale of two symmetrical tails Structural and 
fu.pdf:application/pdf;Snapshot:C\:\\Users\\sydne\\Zotero\\storage\\UZR9NPXE\\PMC2474621.html:text/html},
}

@article{elnaggar_prottrans_2021,
	title = {{ProtTrans}: {Towards} {Cracking} the {Language} of {Lifes} {Code} {Through} {Self}-{Supervised} {Deep} {Learning} and {High} {Performance} {Computing}},
	volume = {44},
	issn = {0162-8828},
	shorttitle = {{ProtTrans}},
	url = {https://www.osti.gov/pages/biblio/1817585},
	doi = {10.1109/TPAMI.2021.3095381},
	abstract = {Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers 
at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 
billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled 
data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction 
of protein secondary structure (3-state accuracy Q3=81\%-87\%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81\%) and membrane vs. water soluble 
(2-state accuracy Q2=91\%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary 
information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we 
released our models at https://github.com/agemagician/ProtTrans.},
	language = {English},
	number = {10},
	urldate = {2023-03-31},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rihawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Bhowmik, 
Debsindhu and Rost, Burkhard},
	month = jul,
	year = {2021},
	note = {Institution: Oak Ridge National Lab. (ORNL), Oak Ridge, TN (United States)
Publisher: IEEE},
	file = {Full Text PDF:C\:\\Users\\sydne\\Zotero\\storage\\I749W4EM\\Elnaggar et al. - 2021 - ProtTrans Towards Cracking the Language of Lifes .pdf:application/pdf},
}

@article{ledford_pioneers_2020,
	title = {Pioneers of revolutionary {CRISPR} gene editing win chemistry {Nobel}},
	volume = {586},
	copyright = {2021 Nature},
	url = {https://www.nature.com/articles/d41586-020-02765-9},
	doi = {10.1038/d41586-020-02765-9},
	abstract = {Emmanuelle Charpentier and Jennifer Doudna share the award for developing the precise genome-editing technology.},
	language = {en},
	number = {7829},
	urldate = {2023-03-31},
	journal = {Nature},
	author = {Ledford, Heidi and Callaway, Ewen},
	month = oct,
	year = {2020},
	note = {Bandiera\_abtest: a
Cg\_type: News
Number: 7829
Publisher: Nature Publishing Group},
	pages = {346--347},
	file = {Full Text PDF:C\:\\Users\\sydne\\Zotero\\storage\\N92MRSHY\\Ledford and Callaway - 2020 - Pioneers of revolutionary CRISPR gene editing win 
.pdf:application/pdf;Snapshot:C\:\\Users\\sydne\\Zotero\\storage\\KBAQUH2Y\\d41586-020-02765-9.html:text/html},
}

@misc{munroe_proteins_2014,
	title = {Proteins},
	url = {https://xkcd.com/1430/},
	urldate = {2023-03-31},
	journal = {xkcd},
	author = {Munroe, Randall},
	year = {2014},
	file = {Snapshot:C\:\\Users\\sydne\\Zotero\\storage\\V44NM2WG\\1430.html:text/html},
}

@misc{peters_deep_2018,
	title = {Deep contextualized word representations},
	url = {https://arxiv.org/abs/1802.05365v2},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary 
across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text 
corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, 
textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types 
of semi-supervision signals.},
	language = {en},
	urldate = {2023-04-02},
	journal = {arXiv.org},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = feb,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\sydne\\Zotero\\storage\\9G73FMX9\\Peters et al. - 2018 - Deep contextualized word representations.pdf:application/pdf},
}

@misc{devlin_bert_2018,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://arxiv.org/abs/1810.04805v2},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, 
BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be 
fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific 
architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 
80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 
to 83.1 (5.1 point absolute improvement).},
	language = {en},
	urldate = {2023-04-02},
	journal = {arXiv.org},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = oct,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\sydne\\Zotero\\storage\\BV9RUVJT\\Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@misc{rao_evaluating_2019,
	title = {Evaluating {Protein} {Transfer} {Learning} with {TAPE}},
	url = {https://arxiv.org/abs/1906.08230v1},
	abstract = {Protein modeling is an increasingly popular area of machine learning research. Semi-supervised learning has emerged as an important paradigm in protein modeling due to the high cost 
of acquiring supervised protein labels, but the current literature is fragmented when it comes to datasets and standardized evaluation techniques. To facilitate progress in this field, we introduce the 
Tasks Assessing Protein Embeddings (TAPE), a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology. We curate tasks into specific training, 
validation, and test splits to ensure that each task tests biologically relevant generalization that transfers to real-life scenarios. We benchmark a range of approaches to semi-supervised protein 
representation learning, which span recent work as well as canonical sequence learning techniques. We find that self-supervised pretraining is helpful for almost all models on all tasks, more than 
doubling performance in some cases. Despite this increase, in several cases features learned by self-supervised pretraining still lag behind features extracted by state-of-the-art non-neural techniques. 
This gap in performance suggests a huge opportunity for innovative architecture design and improved modeling paradigms that better capture the signal in biological sequences. TAPE will help the machine 
learning community focus effort on scientifically relevant problems. Toward this end, all data and code used to run these experiments are available at https://github.com/songlab-cal/tape.},
	language = {en},
	urldate = {2023-04-02},
	journal = {arXiv.org},
	author = {Rao, Roshan and Bhattacharya, Nicholas and Thomas, Neil and Duan, Yan and Chen, Xi and Canny, John and Abbeel, Pieter and Song, Yun S.},
	month = jun,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\sydne\\Zotero\\storage\\YNSAP8UI\\Rao et al. - 2019 - Evaluating Protein Transfer Learning with TAPE.pdf:application/pdf},
}

@article{henikoff_amino_1992,
	title = {Amino acid substitution matrices from protein blocks.},
	volume = {89},
	url = {https://www.pnas.org/doi/10.1073/pnas.89.22.10915},
	doi = {10.1073/pnas.89.22.10915},
	abstract = {Methods for alignment of protein sequences typically measure similarity by using a substitution matrix with scores for all possible exchanges of one amino acid with another. The most 
widely used matrices are based on the Dayhoff model of evolutionary rates. Using a different approach, we have derived substitution matrices from about 2000 blocks of aligned sequence segments 
characterizing more than 500 groups of related proteins. This led to marked improvements in alignments and in searches using queries from each of the groups.},
	number = {22},
	urldate = {2023-04-02},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Henikoff, S and Henikoff, J G},
	month = nov,
	year = {1992},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {10915--10919},
	file = {Full Text PDF:C\:\\Users\\sydne\\Zotero\\storage\\QWN9XXVJ\\Henikoff and Henikoff - 1992 - Amino acid substitution matrices from protein bloc.pdf:application/pdf},
}

@misc{vig_bertology_2020,
	title = {{BERTology} {Meets} {Biology}: {Interpreting} {Attention} in {Protein} {Language} {Models}},
	shorttitle = {{BERTology} {Meets} {Biology}},
	url = {https://arxiv.org/abs/2006.15222v3},
	abstract = {Transformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in 
interpretability. In this work, we demonstrate a set of methods for analyzing protein Transformer models through the lens of attention. We show that attention: (1) captures the folding structure of 
proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, 
and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We find this behavior to be consistent across three Transformer architectures (BERT, ALBERT, XLNet) and 
two distinct protein datasets. We also present a three-dimensional visualization of the interaction between attention and protein structure. Code for visualization and analysis is available at 
https://github.com/salesforce/provis.},
	language = {en},
	urldate = {2023-04-02},
	journal = {arXiv.org},
	author = {Vig, Jesse and Madani, Ali and Varshney, Lav R. and Xiong, Caiming and Socher, Richard and Rajani, Nazneen Fatema},
	month = jun,
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\sydne\\Zotero\\storage\\WDNDBBN2\\Vig et al. - 2020 - BERTology Meets Biology Interpreting Attention in.pdf:application/pdf},
}

@misc{noauthor_third_2017,
	title = {A third of new drugs cause serious problems when more people take them},
	url = {https://www.nbcnews.com/health/health-news/new-drugs-found-cause-side-effects-years-after-approval-n757526},
	abstract = {Almost one-third of new drugs approved by the FDA ended up years later with warnings about unexpected, sometimes life-threatening side effects.},
	language = {en},
	urldate = {2023-04-02},
	journal = {NBC News},
	month = may,
	year = {2017},
	file = {Snapshot:C\:\\Users\\sydne\\Zotero\\storage\\PS3HNC32\\new-drugs-found-cause-side-effects-years-after-approval-n757526.html:text/html},
}

@inproceedings{he_masked_2022,
	address = {New Orleans, LA, USA},
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9879206/},
	doi = {10.1109/CVPR52688.2022.01553},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and 
reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches 
(without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we ﬁnd that masking a high proportion of the input 
image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efﬁciently and effectively: we accelerate training (by 3× or 
more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use 
only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.},
	language = {en},
	urldate = {2023-04-02},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollar, Piotr and Girshick, Ross},
	month = jun,
	year = {2022},
	pages = {15979--15988},
	file = {He et al. - 2022 - Masked Autoencoders Are Scalable Vision Learners.pdf:C\:\\Users\\sydne\\Zotero\\storage\\CW92RCGP\\He et al. - 2022 - Masked Autoencoders Are Scalable Vision 
Learners.pdf:application/pdf},
}

@article{lee_systematic_2021,
	title = {Systematic {Homonym} {Detection} and {Replacement} {Based} on {Contextual} {Word} {Embedding}},
	volume = {53},
	doi = {10.1007/s11063-020-10376-8},
	abstract = {Homonyms are words that share their spelling but differ in meaning and are a common feature in most languages. Homonyms are a source of noise i most text analyses and are difficult 
to detect; numerous studies have been conducted in this regard. However, extant methods typically detect homonyms using a rule-based or statistical-based approach, which requires an answer set, with 
little regard to the semantic meaning of the word. Therefore, we propose a novel approach for the detection of homonyms based on contextual word embedding that allows a word to be understood based on 
the context in which it appears. In this study, we extracted all contextual word embedding vectors of individual words and clustered those vectors using a spherical k-means clustering to detect pairs of 
homonyms. In addition, we developed a homonym replacement method to increase the performance of a document embedding technique, based on the word vector value. We replaced the embedding vectors of 
homonyms with a representative vector based on the respective meaning using the proposed homonym detection method. Experimental results indicate that the proposed method effectively detects homonyms and 
significantly improves the performance of document embedding.},
	journal = {Neural Processing Letters},
	author = {Lee, Younghoon},
	month = feb,
	year = {2021},
	pages = {1--20},
	file = {Full Text PDF:C\:\\Users\\sydne\\Zotero\\storage\\H9BDL4JU\\Lee - 2021 - Systematic Homonym Detection and Replacement Based.pdf:application/pdf},
}

@misc{noauthor_autoencoder_2023,
	title = {Autoencoder},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Autoencoder&oldid=1141727025#Drug_discovery},
	abstract = {An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). An autoencoder learns two functions: an encoding 
function that transforms the input data, and a decoding function that recreates the input data from the encoded representation. The autoencoder learns an efficient representation (encoding) for a set of 
data, typically for dimensionality reduction.
Variants exist, aiming to force the learned representations to assume useful properties. Examples are regularized autoencoders (Sparse, Denoising and Contractive), which are effective in learning 
representations for subsequent classification tasks, and Variational autoencoders, with applications as generative models. Autoencoders are applied to many problems, including facial recognition, 
feature detection, anomaly detection and acquiring the meaning of words. Autoencoders are also generative models which can randomly generate new data that is similar to the input data (training data).},
	language = {en},
	urldate = {2023-04-02},
	journal = {Wikipedia},
	month = feb,
	year = {2023},
	note = {Page Version ID: 1141727025},
	file = {Snapshot:C\:\\Users\\sydne\\Zotero\\storage\\QQHFZ4PU\\Autoencoder.html:text/html},
}

@misc{noauthor_multilayer_2023,
	title = {Multilayer perceptron},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Multilayer_perceptron&oldid=1147594876},
	abstract = {A multilayer perceptron (MLP) is a fully connected class of feedforward artificial neural network (ANN). The term MLP is used ambiguously, sometimes loosely to mean any feedforward 
ANN, sometimes strictly to refer to networks composed of multiple layers of perceptrons (with threshold activation); see § Terminology. Multilayer perceptrons are sometimes colloquially referred to as 
"vanilla" neural networks, especially when they have a single hidden layer.An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input 
nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a chain rule based supervised learning technique called backpropagation or reverse mode of automatic differentiation 
for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.},
	language = {en},
	urldate = {2023-04-02},
	journal = {Wikipedia},
	month = mar,
	year = {2023},
	note = {Page Version ID: 1147594876},
	file = {Snapshot:C\:\\Users\\sydne\\Zotero\\storage\\5QSZAZ8X\\Multilayer_perceptron.html:text/html},
}

@article{bepler_learning_2021,
	title = {Learning the protein language: {Evolution}, structure, and function},
	volume = {12},
	issn = {2405-4712},
	shorttitle = {Learning the protein language},
	url = {https://www.sciencedirect.com/science/article/pii/S2405471221002039},
	doi = {10.1016/j.cels.2021.05.017},
	abstract = {Language models have recently emerged as a powerful machine-learning approach for distilling information from massive protein sequence databases. From readily available sequence data 
alone, these models discover evolutionary, structural, and functional organization across protein space. Using language models, we can encode amino-acid sequences into distributed vector representations 
that capture their structural and functional properties, as well as evaluate the evolutionary fitness of sequence variants. We discuss recent advances in protein language modeling and their applications 
to downstream protein property prediction problems. We then consider how these models can be enriched with prior biological knowledge and introduce an approach for encoding protein structural knowledge 
into the learned representations. The knowledge distilled by these models allows us to improve downstream function prediction through transfer learning. Deep protein language models are revolutionizing 
protein biology. They suggest new ways to approach protein and therapeutic design. However, further developments are needed to encode strong biological priors into protein language models and to 
increase their accessibility to the broader community.},
	language = {en},
	number = {6},
	urldate = {2023-04-02},
	journal = {Cell Systems},
	author = {Bepler, Tristan and Berger, Bonnie},
	month = jun,
	year = {2021},
	keywords = {contact prediction, deep neural networks, inductive bias, language models, natural language processing, protein sequences, proteins, transfer learning, transmembrane region 
prediction},
	pages = {654--669.e3},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\sydne\\Zotero\\storage\\LL8TL8T4\\Bepler and Berger - 2021 - Learning the protein language Evolution, structur.pdf:application/pdf;ScienceDirect 
Snapshot:C\:\\Users\\sydne\\Zotero\\storage\\5JVSHYAH\\S2405471221002039.html:text/html},
}

@article{gatherer_peptide_2007,
	title = {Peptide {Vocabulary} {Analysis} {Reveals} {Ultra}-{Conservation} and {Homonymity} in {Protein} {Sequences}},
	volume = {1},
	issn = {1177-9322},
	url = {https://doi.org/10.4137/BBI.S415},
	doi = {10.4137/BBI.S415},
	abstract = {A new algorithm is presented for vocabulary analysis (word detection) in texts of human origin. It performs at 60\%?70\% overall accuracy and greater than 80\% accuracy for longer 
words, and approximately 85\% sensitivity on Alice in Wonderland, a considerable improvement on previous methods. When applied to protein sequences, it detects short sequences analogous to words in 
human texts, i.e. intolerant to changes in spelling (mutation), and relatively context-independent in their meaning (function). Some of these are homonyms of up to 7 amino acids, which can assume 
different structures in different proteins. Others are ultra-conserved stretches of up to 18 amino acids within proteins of less than 40\% overall identity, reflecting extreme constraint or convergent 
evolution. Different species are found to have qualitatively different major peptide vocabularies, e.g. some are dominated by large gene families, while others are rich in simple repeats or dominated by 
internally repetitive proteins. This suggests the possibility of a peptide vocabulary signature, analogous to genome signatures in DNA. Homonyms may be useful in detecting convergent evolution and 
positive selection in protein evolution. Ultra-conserved words may be useful in identifying structures intolerant to substitution over long periods of evolutionary time.},
	language = {en},
	urldate = {2023-04-02},
	journal = {Bioinformatics and Biology Insights},
	author = {Gatherer, Derek},
	month = jan,
	year = {2007},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {BBI.S415},
	file = {SAGE PDF Full Text:C\:\\Users\\sydne\\Zotero\\storage\\L7AW9YDY\\Gatherer - 2007 - Peptide Vocabulary Analysis Reveals Ultra-Conserva.pdf:application/pdf},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect 
the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions 
entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 
BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model 
establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that 
the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2023-04-02},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\sydne\\Zotero\\storage\\QLY3UAIQ\\Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org 
Snapshot:C\:\\Users\\sydne\\Zotero\\storage\\ENG6CLWM\\1706.html:text/html},
}

@misc{bepler_learning_2019,
	title = {Learning protein sequence embeddings using information from structure},
	url = {http://arxiv.org/abs/1902.08661},
	doi = {10.48550/arXiv.1902.08661},
	abstract = {Inferring the structural properties of a protein from its amino acid sequence is a challenging yet important problem in biology. Structures are not known for the vast majority of 
protein sequences, but structure is critical for understanding function. Existing approaches for detecting structural similarity between proteins from sequence are unable to recognize and exploit 
structural patterns when sequences have diverged too far, limiting our ability to transfer knowledge between structurally related proteins. We newly approach this problem through the lens of 
representation learning. We introduce a framework that maps any protein sequence to a sequence of vector embeddings --- one per amino acid position --- that encode structural information. We train 
bidirectional long short-term memory (LSTM) models on protein sequences with a two-part feedback mechanism that incorporates information from (i) global structural similarity between proteins and (ii) 
pairwise residue contact maps for individual proteins. To enable learning from structural similarity information, we define a novel similarity measure between arbitrary-length sequences of vector 
embeddings based on a soft symmetric alignment (SSA) between them. Our method is able to learn useful position-specific embeddings despite lacking direct observations of position-level correspondence 
between sequences. We show empirically that our multi-task framework outperforms other sequence-based methods and even a top-performing structure-based alignment method when predicting structural 
similarity, our goal. Finally, we demonstrate that our learned embeddings can be transferred to other protein sequence problems, improving the state-of-the-art in transmembrane domain prediction.},
	urldate = {2023-04-02},
	publisher = {arXiv},
	author = {Bepler, Tristan and Berger, Bonnie},
	month = oct,
	year = {2019},
	note = {arXiv:1902.08661 [cs, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules, Statistics - Machine Learning},
	annote = {Comment: 17 pages, 3 figures, 8 tables, proceedings of ICLR 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\sydne\\Zotero\\storage\\ARWP7URV\\Bepler and Berger - 2019 - Learning protein sequence embeddings using informa.pdf:application/pdf;arXiv.org 
Snapshot:C\:\\Users\\sydne\\Zotero\\storage\\VIYCDRP8\\1902.html:text/html},
}

@misc{avildsen_karate_1984,
	title = {The {Karate} {Kid}},
	author = {Avildsen, John G.},
	year = {1984},
}

@misc{reiner_princess_1987,
	title = {The {Princess} {Bride}},
	publisher = {20th Century Studios},
	author = {Reiner, Robert C.},
	year = {1987},
}

@incollection{mount_bioinformatics_nodate,
	title = {Bioinformatics {Sequence} and {Genome} {Analysis}},
	publisher = {Cold Spring Harbor Laboratory Press},
	author = {Mount, David W.},
	pages = {281--335},
}

